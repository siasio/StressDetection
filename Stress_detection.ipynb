{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stress-detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Library for sound processing with the Wav2Vec2 models: https://github.com/jonatasgrosman/huggingsound <br>\n",
        "API of the Russian National Corpus: https://github.com/kunansy/RNC <br>\n",
        "Not fine-tuned large Wav2Vec2 model pretrained on common_voice dataset for 53 languages: \"facebook/wav2vec2-large-xlsr-53\"<br>\n",
        "Fine-tuned Wav2Vec2 model which is most probably useless for us because it uses a token set containing characters of the cyrillic alphabet and we want to also use tokens which mark the lexical stress: \"jonatasgrosman/wav2vec2-large-xlsr-53-russian\"<br>\n",
        "Training arguments (might be useful for performing ablation studies): https://huggingface.co/transformers/v4.4.2/_modules/transformers/training_args.html"
      ],
      "metadata": {
        "id": "-0upsp9judfq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Google drive\n",
        "Mounting google drive is necessary for working with files saved there. I shared the folder with RNC data with you. To work with the data folder, go to the \"Shared with me\" folder on your google drive, right-click on the RussianNationalCorpus and press \"Add shortcut to Drive\"."
      ],
      "metadata": {
        "id": "NZkKRaECVwVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "import os\n",
        "DATA_PATH = \"/content/drive/MyDrive/RussianNationalCorpus\"\n",
        "DOWNLOADING_PATH = os.path.join(DATA_PATH, 'download_examples.py')\n",
        "TRAINING_PATH = os.path.join(DATA_PATH, 'run_training.py')\n",
        "EVALUATION_PATH = os.path.join(DATA_PATH, 'evaluate_model.py')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jY0oHFoll87",
        "outputId": "a773938f-95d1-4158-e9f3-686979b6cc23"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pip install libraries rnc and hugging sound\n",
        "Attention: after installing the hugginsound library, you will most probably need to change a library file called trainer.py. In colab you will find it located in usr/local/lib/python3.7/dist-packages/hugginsound/trainer.py (to find the usr folder, open the file browser in the left panel and press two dots above the sample_data folder to reach the root directory). The change you need to make, is replacing self.use_amp with self.use_cuda_amp in lines 434 and 451.\n",
        "\n",
        "After making the changes, you need to reinstall the library. In colab, you need to restart runtime first and then run the pip install command again.\n",
        "\n",
        "Making the changes is not required if you only want to run the evaluation."
      ],
      "metadata": {
        "id": "LVPjsx8hV64a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2sSIJ-OayjEZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b83a09d0-df97-4f0e-ff7b-642cdc7fe2aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rnc in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: types-aiofiles<0.9.0,>=0.8.4 in /usr/local/lib/python3.7/dist-packages (from rnc) (0.8.8)\n",
            "Requirement already satisfied: beautifulsoup4<4.11.0,>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from rnc) (4.10.0)\n",
            "Requirement already satisfied: lxml<4.9.0,>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from rnc) (4.8.0)\n",
            "Requirement already satisfied: aiohttp<3.9.0,>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from rnc) (3.8.1)\n",
            "Requirement already satisfied: aiofiles<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from rnc) (0.8.0)\n",
            "Requirement already satisfied: bs4<0.1.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from rnc) (0.0.1)\n",
            "Requirement already satisfied: ujson<5.2.0,>=5.1.0 in /usr/local/lib/python3.7/dist-packages (from rnc) (5.1.0)\n",
            "Requirement already satisfied: types-ujson<4.3.0,>=4.2.1 in /usr/local/lib/python3.7/dist-packages (from rnc) (4.2.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp<3.9.0,>=3.8.1->rnc) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<3.9.0,>=3.8.1->rnc) (2.0.12)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<3.9.0,>=3.8.1->rnc) (21.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp<3.9.0,>=3.8.1->rnc) (1.2.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<3.9.0,>=3.8.1->rnc) (0.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp<3.9.0,>=3.8.1->rnc) (4.1.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp<3.9.0,>=3.8.1->rnc) (1.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<3.9.0,>=3.8.1->rnc) (1.7.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp<3.9.0,>=3.8.1->rnc) (6.0.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.7/dist-packages (from beautifulsoup4<4.11.0,>=4.10.0->rnc) (2.3.2.post1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp<3.9.0,>=3.8.1->rnc) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: huggingsound in /usr/local/lib/python3.7/dist-packages (0.1.4)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.16.2 in /usr/local/lib/python3.7/dist-packages (from huggingsound) (4.20.0)\n",
            "Requirement already satisfied: torch<2.0.0,>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from huggingsound) (1.11.0+cu113)\n",
            "Requirement already satisfied: jiwer<3.0.0,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from huggingsound) (2.3.0)\n",
            "Requirement already satisfied: librosa<0.9.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from huggingsound) (0.8.1)\n",
            "Requirement already satisfied: datasets<2.0.0,>=1.18.3 in /usr/local/lib/python3.7/dist-packages (from huggingsound) (1.18.4)\n",
            "Requirement already satisfied: llvmlite<0.37.0,>=0.36.0 in /usr/local/lib/python3.7/dist-packages (from huggingsound) (0.36.0)\n",
            "Requirement already satisfied: numba<0.54.0,>=0.53.1 in /usr/local/lib/python3.7/dist-packages (from huggingsound) (0.53.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.18.3->huggingsound) (2.23.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.18.3->huggingsound) (2022.5.0)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.18.3->huggingsound) (6.0.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.18.3->huggingsound) (3.8.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.18.3->huggingsound) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.18.3->huggingsound) (1.21.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.18.3->huggingsound) (3.0.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.18.3->huggingsound) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.18.3->huggingsound) (4.11.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.18.3->huggingsound) (0.7.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.18.3->huggingsound) (0.18.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.18.3->huggingsound) (1.3.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.18.3->huggingsound) (0.3.5.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.18.3->huggingsound) (0.70.13)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets<2.0.0,>=1.18.3->huggingsound) (4.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets<2.0.0,>=1.18.3->huggingsound) (3.7.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets<2.0.0,>=1.18.3->huggingsound) (6.0)\n",
            "Requirement already satisfied: python-Levenshtein==0.12.2 in /usr/local/lib/python3.7/dist-packages (from jiwer<3.0.0,>=2.3.0->huggingsound) (0.12.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein==0.12.2->jiwer<3.0.0,>=2.3.0->huggingsound) (57.4.0)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.9.0,>=0.8.1->huggingsound) (1.0.2)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa<0.9.0,>=0.8.1->huggingsound) (0.2.2)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.9.0,>=0.8.1->huggingsound) (2.1.9)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa<0.9.0,>=0.8.1->huggingsound) (0.10.3.post1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.9.0,>=0.8.1->huggingsound) (1.4.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.9.0,>=0.8.1->huggingsound) (1.6.0)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa<0.9.0,>=0.8.1->huggingsound) (1.1.0)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.9.0,>=0.8.1->huggingsound) (4.4.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets<2.0.0,>=1.18.3->huggingsound) (3.0.9)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa<0.9.0,>=0.8.1->huggingsound) (1.4.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets<2.0.0,>=1.18.3->huggingsound) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets<2.0.0,>=1.18.3->huggingsound) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets<2.0.0,>=1.18.3->huggingsound) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets<2.0.0,>=1.18.3->huggingsound) (1.25.11)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->librosa<0.9.0,>=0.8.1->huggingsound) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa<0.9.0,>=0.8.1->huggingsound) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa<0.9.0,>=0.8.1->huggingsound) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa<0.9.0,>=0.8.1->huggingsound) (2.21)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.16.2->huggingsound) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.16.2->huggingsound) (0.12.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.18.3->huggingsound) (21.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.18.3->huggingsound) (1.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.18.3->huggingsound) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.18.3->huggingsound) (6.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.18.3->huggingsound) (1.7.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.18.3->huggingsound) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.18.3->huggingsound) (2.0.12)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.18.3->huggingsound) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets<2.0.0,>=1.18.3->huggingsound) (3.8.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets<2.0.0,>=1.18.3->huggingsound) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets<2.0.0,>=1.18.3->huggingsound) (2.8.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install rnc\n",
        "!pip install huggingsound"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download samples if they are not downloaded yet\n",
        "Perhaps, we need to import the nest_asyncio library. Without it, colab has problems with managing nested asynchronous processes."
      ],
      "metadata": {
        "id": "MtBXiOEWXeHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "!python $DOWNLOADING_PATH"
      ],
      "metadata": {
        "id": "rFbYH9vIynJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run training\n",
        "Run first the --help command to learn which arguments you should provide"
      ],
      "metadata": {
        "id": "AL-YGEGCXxyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python $TRAINING_PATH --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wp_h5aaquYDQ",
        "outputId": "f5b2052a-851c-43f0-87b6-beb705370c6a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: run_training.py [-h] [--data_path DATA_PATH] [--train_data TRAIN_DATA]\n",
            "                       [--eval_data EVAL_DATA] [--use_model_from_the_web]\n",
            "                       [--model_folder MODEL_FOLDER]\n",
            "                       [--model_web_location MODEL_WEB_LOCATION]\n",
            "                       [--output_dir OUTPUT_DIR] [--not_overwrite_output_dir]\n",
            "                       [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --data_path DATA_PATH\n",
            "                        Path to the folder containing the csv file and the\n",
            "                        model folder. Defaults to empty string '' which might\n",
            "                        work if data sits in the current working directory. If\n",
            "                        it doesn't work, then you need to specify the path.\n",
            "  --train_data TRAIN_DATA\n",
            "                        Name of the csv file with the training data, needs to\n",
            "                        be located in the data path.\n",
            "  --eval_data EVAL_DATA\n",
            "                        Name of the csv file with the evaluation data, needs\n",
            "                        to be located in the data path. Defaults to None (not\n",
            "                        using evaluation during training).\n",
            "  --use_model_from_the_web\n",
            "                        If set to False, use --model_folder If set to True,\n",
            "                        use --model_web_location\n",
            "  --model_folder MODEL_FOLDER\n",
            "                        Name of the folder containing the model, needs to be\n",
            "                        located inside the data path folder.\n",
            "  --model_web_location MODEL_WEB_LOCATION\n",
            "                        Name of the web location containing the model, either\n",
            "                        fine-tuned or only pretrained. Note that for a fine-\n",
            "                        tuned model a token set is already defined.\n",
            "  --output_dir OUTPUT_DIR\n",
            "                        The output directory for the model. Could be the same\n",
            "                        as the --model_folder\n",
            "  --not_overwrite_output_dir\n",
            "                        If this flag is provided, the --output_dir must point\n",
            "                        to an empty or non-existing directory (note that by\n",
            "                        default it points to the model folder).\n",
            "  --num_train_epochs NUM_TRAIN_EPOCHS\n",
            "                        Training will start from the next epoch compared to\n",
            "                        the one on which it finished previously. Therefore, we\n",
            "                        need to increase this number each time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python $TRAINING_PATH --data_path $DATA_PATH"
      ],
      "metadata": {
        "id": "LgpsyfRDTLdL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dd40e4f-ad8f-4809-e14c-edf89a86fcf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "06/20/2022 00:50:10 - INFO - huggingsound.speech_recognition.model - Loading model...\n",
            "06/20/2022 00:50:15 - INFO - rnc - Requested to 'https://processing.ruscorpora.ru/search.xml' [0;1) with params {'env': 'alpha', 'api': '1.0', 'lang': 'ru', 'dpp': 5, 'spd': 10, 'text': 'lexgramm', 'out': 'normal', 'sort': 'i_grtagging', 'nodia': 0, 'lex1': 'и', 'mode': 'murco'}\n",
            "06/20/2022 00:50:15 - DEBUG - rnc - Worker-1: Requested to 'https://processing.ruscorpora.ru/search.xml' with '{'env': 'alpha', 'api': '1.0', 'lang': 'ru', 'dpp': 5, 'spd': 10, 'text': 'lexgramm', 'out': 'normal', 'sort': 'i_grtagging', 'nodia': 0, 'lex1': 'и', 'mode': 'murco', 'p': 0}'\n",
            "06/20/2022 00:50:17 - DEBUG - rnc - Worker-1: Received from 'https://processing.ruscorpora.ru/search.xml' with '{'env': 'alpha', 'api': '1.0', 'lang': 'ru', 'dpp': 5, 'spd': 10, 'text': 'lexgramm', 'out': 'normal', 'sort': 'i_grtagging', 'nodia': 0, 'lex1': 'и', 'mode': 'murco', 'p': 0}'\n",
            "06/20/2022 00:50:17 - INFO - rnc - Request was successfully completed\n",
            "06/20/2022 00:50:17 - INFO - rnc - Coro executing time: 2.31\n",
            "#э##а##аоо####э####о####э###у#э##э##э#и####э#####э####э###о#у#\n",
            "06/20/2022 00:50:17 - WARNING - root - blank_token <pad> not in provided tokens. It will be added to the list of tokens\n",
            "06/20/2022 00:50:17 - WARNING - root - silence_token | not in provided tokens. It will be added to the list of tokens\n",
            "06/20/2022 00:50:17 - WARNING - root - unk_token <unk> not in provided tokens. It will be added to the list of tokens\n",
            "06/20/2022 00:50:17 - WARNING - root - bos_token <s> not in provided tokens. It will be added to the list of tokens\n",
            "06/20/2022 00:50:17 - WARNING - root - eos_token </s> not in provided tokens. It will be added to the list of tokens\n",
            "06/20/2022 00:50:17 - INFO - huggingsound.speech_recognition.model - Loading training data...\n",
            "06/20/2022 00:50:17 - INFO - huggingsound.speech_recognition.model - Converting data format...\n",
            "06/20/2022 00:50:17 - INFO - huggingsound.speech_recognition.model - Preparing data input and labels...\n",
            "06/20/2022 00:50:17 - WARNING - datasets.fingerprint - Parameter 'function'=<function SpeechRecognitionModel._prepare_dataset_for_finetuning.<locals>.__process_dataset_sample at 0x7fb09b225680> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "187ex [06:18,  2.02s/ex]\n",
            "06/20/2022 00:56:36 - INFO - huggingsound.speech_recognition.model - Starting fine-tuning process...\n",
            "06/20/2022 00:56:36 - INFO - huggingsound.trainer - Getting dataset stats...\n",
            "06/20/2022 00:56:49 - INFO - huggingsound.trainer - Training dataset size: 187 samples, 0.7284649826388894 hours\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1619: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5.Please use the equivalent `freeze_feature_encoder` method instead.\n",
            "  FutureWarning,\n",
            "06/20/2022 00:56:53 - INFO - huggingsound.trainer - Building trainer...\n",
            "06/20/2022 00:56:53 - INFO - huggingsound.trainer - Starting training...\n",
            "Feature extractor saved in /content/drive/MyDrive/RussianNationalCorpus/mature_model21/preprocessor_config.json\n",
            "tokenizer config file saved in /content/drive/MyDrive/RussianNationalCorpus/mature_model21/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/RussianNationalCorpus/mature_model21/special_tokens_map.json\n",
            "Loading model from /content/drive/MyDrive/RussianNationalCorpus/mature_model2.\n",
            "The following columns in the training set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: length. If length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
            "***** Running training *****\n",
            "  Num examples = 187\n",
            "  Num Epochs = 6\n",
            "  Instantaneous batch size per device = 2\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 564\n",
            "  Continuing training from checkpoint, will skip to saved global_step\n",
            "  Continuing training from epoch 3\n",
            "  Continuing training from global step 321\n",
            "  Will skip the first 3 epochs then the first 39 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n",
            "Skipping the first batches:   0% 0/39 [00:00<?, ?it/s]\n",
            "Skipping the first batches: 100% 39/39 [00:21<00:00,  6.37it/s]Didn't find an RNG file, if you are resuming a training that was launched in a distributed fashion, reproducibility is not guaranteed.\n",
            "Skipping the first batches: 100% 39/39 [00:22<00:00,  1.77it/s]\n",
            "\n",
            " 57% 322/564 [01:26<01:04,  3.73it/s]\u001b[A\n",
            " 57% 323/564 [02:27<02:09,  1.86it/s]\u001b[A\n",
            " 57% 324/564 [03:27<03:38,  1.10it/s]\u001b[A\n",
            " 58% 325/564 [04:23<05:34,  1.40s/it]\u001b[A\n",
            " 58% 326/564 [05:18<08:12,  2.07s/it]\u001b[A\n",
            " 58% 327/564 [06:05<11:19,  2.87s/it]\u001b[A\n",
            " 58% 328/564 [06:39<14:15,  3.62s/it]\u001b[A\n",
            " 58% 329/564 [08:35<29:02,  7.41s/it]\u001b[A\n",
            " 59% 330/564 [10:24<47:10, 12.10s/it]\u001b[A\n",
            " 59% 331/564 [12:10<1:09:27, 17.89s/it]\u001b[A\n",
            " 59% 332/564 [13:56<1:36:56, 25.07s/it]\u001b[A\n",
            " 59% 333/564 [15:37<2:06:32, 32.87s/it]\u001b[A\n",
            " 59% 334/564 [17:14<2:37:44, 41.15s/it]\u001b[A\n",
            " 59% 335/564 [18:47<3:07:49, 49.21s/it]\u001b[A\n",
            " 60% 336/564 [20:17<3:35:27, 56.70s/it]\u001b[A\n",
            " 60% 337/564 [21:44<3:58:32, 63.05s/it]\u001b[A\n",
            " 60% 338/564 [23:06<4:13:39, 67.34s/it]\u001b[A\n",
            " 60% 339/564 [24:23<4:21:35, 69.76s/it]\u001b[A\n",
            " 60% 340/564 [25:39<4:26:12, 71.31s/it]\u001b[A\n",
            " 60% 341/564 [26:53<4:27:47, 72.05s/it]\u001b[A\n",
            " 61% 342/564 [28:05<4:26:03, 71.91s/it]\u001b[A\n",
            " 61% 343/564 [29:14<4:22:29, 71.26s/it]\u001b[A\n",
            " 61% 344/564 [30:20<4:14:54, 69.52s/it]\u001b[A\n",
            " 61% 345/564 [31:24<4:07:45, 67.88s/it]\u001b[A\n",
            " 61% 346/564 [32:24<3:58:44, 65.71s/it]\u001b[A\n",
            " 62% 347/564 [33:21<3:48:30, 63.18s/it]\u001b[A\n",
            " 62% 348/564 [34:15<3:37:47, 60.50s/it]\u001b[A\n",
            " 62% 349/564 [35:07<3:26:49, 57.72s/it]\u001b[A\n",
            " 62% 350/564 [35:53<3:14:22, 54.50s/it]\u001b[A\n",
            " 62% 351/564 [36:33<2:57:12, 49.92s/it]\u001b[A\n",
            " 62% 352/564 [38:24<4:00:50, 68.16s/it]\u001b[A\n",
            " 63% 353/564 [40:17<4:47:06, 81.64s/it]\u001b[A\n",
            " 63% 354/564 [42:03<5:11:47, 89.09s/it]\u001b[A\n",
            " 63% 355/564 [43:40<5:17:59, 91.29s/it]\u001b[A\n",
            " 63% 356/564 [45:13<5:18:57, 92.01s/it]\u001b[A\n",
            " 63% 357/564 [46:46<5:18:08, 92.21s/it]\u001b[A\n",
            " 63% 358/564 [48:15<5:13:36, 91.34s/it]\u001b[A\n",
            " 64% 359/564 [49:41<5:05:59, 89.56s/it]\u001b[A\n",
            " 64% 360/564 [51:03<4:57:30, 87.50s/it]\u001b[A\n",
            " 64% 361/564 [52:24<4:49:10, 85.47s/it]\u001b[A\n",
            " 64% 362/564 [53:43<4:41:07, 83.50s/it]\u001b[A\n",
            " 64% 363/564 [55:01<4:34:27, 81.93s/it]\u001b[A\n",
            " 65% 364/564 [56:14<4:23:40, 79.10s/it]\u001b[A\n",
            " 65% 365/564 [57:24<4:13:34, 76.45s/it]\u001b[A\n",
            " 65% 366/564 [58:29<4:00:57, 73.02s/it]\u001b[A\n",
            " 65% 367/564 [59:30<3:48:08, 69.49s/it]\u001b[A\n",
            " 65% 368/564 [1:00:29<3:36:32, 66.29s/it]\u001b[A\n",
            " 65% 369/564 [1:01:28<3:28:00, 64.00s/it]\u001b[A\n",
            " 66% 370/564 [1:02:23<3:18:20, 61.34s/it]\u001b[A\n",
            " 66% 371/564 [1:03:14<3:07:13, 58.20s/it]\u001b[A\n",
            " 66% 372/564 [1:04:02<2:56:30, 55.16s/it]\u001b[A\n",
            " 66% 373/564 [1:04:45<2:44:14, 51.60s/it]\u001b[A\n",
            " 66% 374/564 [1:05:23<2:30:42, 47.59s/it]\u001b[A\n",
            " 66% 375/564 [1:07:13<3:28:40, 66.25s/it]\u001b[A\n",
            " 67% 376/564 [1:07:43<2:53:08, 55.26s/it]\u001b[A\n",
            " 67% 377/564 [1:09:38<3:48:13, 73.23s/it]\u001b[A\n",
            " 67% 378/564 [1:11:24<4:17:27, 83.05s/it]\u001b[A\n",
            " 67% 379/564 [1:13:07<4:34:10, 88.92s/it]\u001b[A\n",
            " 67% 380/564 [1:14:45<4:41:03, 91.65s/it]\u001b[A\n",
            " 68% 381/564 [1:16:22<4:45:13, 93.52s/it]\u001b[A\n",
            " 68% 382/564 [1:17:59<4:46:05, 94.32s/it]\u001b[A\n",
            " 68% 383/564 [1:19:33<4:44:40, 94.37s/it]\u001b[A\n",
            " 68% 384/564 [1:21:02<4:38:12, 92.74s/it]\u001b[A\n",
            " 68% 385/564 [1:22:26<4:28:39, 90.05s/it]\u001b[A\n",
            " 68% 386/564 [1:23:48<4:20:01, 87.65s/it]\u001b[A\n",
            " 69% 387/564 [1:25:07<4:11:19, 85.20s/it]\u001b[A\n",
            " 69% 388/564 [1:26:23<4:01:48, 82.44s/it]\u001b[A\n",
            " 69% 389/564 [1:27:37<3:52:31, 79.72s/it]\u001b[A\n",
            " 69% 390/564 [1:28:51<3:46:08, 77.98s/it]\u001b[A\n",
            " 69% 391/564 [1:29:59<3:36:21, 75.04s/it]\u001b[A\n",
            " 70% 392/564 [1:31:04<3:26:57, 72.19s/it]\u001b[A\n",
            " 70% 393/564 [1:32:08<3:18:39, 69.71s/it]\u001b[A\n",
            " 70% 394/564 [1:33:12<3:11:58, 67.76s/it]\u001b[A\n",
            " 70% 395/564 [1:34:12<3:05:05, 65.72s/it]\u001b[A\n",
            " 70% 396/564 [1:35:13<2:59:49, 64.22s/it]\u001b[A\n",
            " 70% 397/564 [1:36:14<2:55:32, 63.07s/it]\u001b[A\n",
            " 71% 398/564 [1:37:11<2:50:01, 61.45s/it]\u001b[A\n",
            " 71% 399/564 [1:37:48<2:28:52, 54.14s/it]\u001b[A\n",
            " 71% 400/564 [1:39:50<3:22:58, 74.26s/it]\u001b[A\n",
            "\u001b[A{'loss': 173.2954, 'learning_rate': 0.0003, 'epoch': 4.26}\n",
            "\n",
            " 71% 400/564 [1:39:50<3:22:58, 74.26s/it]\u001b[A\n",
            " 71% 401/564 [1:41:46<3:56:11, 86.94s/it]\u001b[A\n",
            " 71% 402/564 [1:43:40<4:16:21, 94.95s/it]\u001b[A\n",
            " 71% 403/564 [1:45:28<4:25:36, 98.98s/it]\u001b[A\n",
            " 72% 404/564 [1:47:17<4:31:32, 101.83s/it]\u001b[A\n",
            " 72% 405/564 [1:49:01<4:31:34, 102.48s/it]\u001b[A\n",
            " 72% 406/564 [1:50:42<4:28:51, 102.10s/it]\u001b[A\n",
            " 72% 407/564 [1:52:12<4:18:00, 98.60s/it] \u001b[A\n",
            " 72% 408/564 [1:53:37<4:05:26, 94.40s/it]\u001b[A\n",
            " 73% 409/564 [1:54:58<3:53:33, 90.41s/it]\u001b[A\n",
            " 73% 410/564 [1:56:14<3:41:08, 86.16s/it]\u001b[A\n",
            " 73% 411/564 [1:57:29<3:31:23, 82.90s/it]\u001b[A\n",
            " 73% 412/564 [1:58:44<3:24:00, 80.53s/it]\u001b[A\n",
            " 73% 413/564 [1:59:57<3:16:46, 78.19s/it]\u001b[A\n",
            " 73% 414/564 [2:01:07<3:09:02, 75.62s/it]\u001b[A\n",
            " 74% 415/564 [2:02:12<2:59:54, 72.45s/it]\u001b[A\n",
            " 74% 416/564 [2:03:12<2:49:33, 68.74s/it]\u001b[A\n",
            " 74% 417/564 [2:04:10<2:40:22, 65.46s/it]\u001b[A\n",
            " 74% 418/564 [2:05:07<2:33:35, 63.12s/it]\u001b[A\n",
            " 74% 419/564 [2:05:59<2:24:01, 59.60s/it]\u001b[A\n",
            " 74% 420/564 [2:06:45<2:13:09, 55.48s/it]\u001b[A\n",
            " 75% 421/564 [2:07:29<2:04:07, 52.08s/it]\u001b[A\n",
            " 75% 422/564 [2:08:05<1:52:10, 47.40s/it]\u001b[A\n",
            " 75% 423/564 [2:10:00<2:38:48, 67.58s/it]\u001b[A\n",
            " 75% 424/564 [2:11:43<3:02:16, 78.12s/it]\u001b[A\n",
            " 75% 425/564 [2:13:27<3:19:25, 86.08s/it]\u001b[A\n",
            " 76% 426/564 [2:15:08<3:28:00, 90.44s/it]\u001b[A\n",
            " 76% 427/564 [2:16:45<3:30:43, 92.29s/it]\u001b[A\n",
            " 76% 428/564 [2:18:17<3:29:09, 92.28s/it]\u001b[A\n",
            " 76% 429/564 [2:19:43<3:23:30, 90.45s/it]\u001b[A\n",
            " 76% 430/564 [2:21:09<3:19:19, 89.25s/it]\u001b[A\n",
            " 76% 431/564 [2:22:31<3:12:37, 86.90s/it]\u001b[A\n",
            " 77% 432/564 [2:23:52<3:07:15, 85.12s/it]\u001b[A\n",
            " 77% 433/564 [2:25:13<3:03:18, 83.96s/it]\u001b[A\n",
            " 77% 434/564 [2:26:30<2:57:27, 81.91s/it]\u001b[A\n",
            " 77% 435/564 [2:27:46<2:52:25, 80.20s/it]\u001b[A\n",
            " 77% 436/564 [2:29:02<2:48:00, 78.76s/it]\u001b[A\n",
            " 77% 437/564 [2:30:14<2:42:38, 76.83s/it]\u001b[A\n",
            " 78% 438/564 [2:31:25<2:37:46, 75.13s/it]\u001b[A\n",
            " 78% 439/564 [2:32:31<2:30:55, 72.44s/it]\u001b[A\n",
            " 78% 440/564 [2:33:29<2:20:43, 68.09s/it]\u001b[A\n",
            " 78% 441/564 [2:34:24<2:11:35, 64.19s/it]\u001b[A\n",
            " 78% 442/564 [2:35:16<2:03:05, 60.54s/it]\u001b[A\n",
            " 79% 443/564 [2:36:07<1:56:12, 57.62s/it]\u001b[A\n",
            " 79% 444/564 [2:36:55<1:49:01, 54.52s/it]\u001b[A\n",
            " 79% 445/564 [2:37:37<1:41:04, 50.96s/it]\u001b[A\n",
            " 79% 446/564 [2:39:30<2:16:31, 69.42s/it]\u001b[A\n",
            " 79% 447/564 [2:41:18<2:37:50, 80.95s/it]\u001b[A\n",
            " 79% 448/564 [2:43:03<2:50:57, 88.43s/it]\u001b[A\n",
            " 80% 449/564 [2:44:48<2:58:50, 93.31s/it]\u001b[A\n",
            " 80% 450/564 [2:46:23<2:58:16, 93.83s/it]\u001b[A\n",
            " 80% 451/564 [2:47:55<2:55:34, 93.22s/it]\u001b[A\n",
            " 80% 452/564 [2:49:26<2:53:01, 92.69s/it]\u001b[A\n",
            " 80% 453/564 [2:50:52<2:47:21, 90.46s/it]\u001b[A\n",
            " 80% 454/564 [2:52:18<2:43:45, 89.32s/it]\u001b[A\n",
            " 81% 455/564 [2:53:39<2:37:27, 86.68s/it]\u001b[A\n",
            " 81% 456/564 [2:54:57<2:31:10, 83.98s/it]\u001b[A\n",
            " 81% 457/564 [2:56:13<2:25:45, 81.74s/it]\u001b[A\n",
            " 81% 458/564 [2:57:27<2:20:07, 79.31s/it]\u001b[A\n",
            " 81% 459/564 [2:58:38<2:14:31, 76.87s/it]\u001b[A\n",
            " 82% 460/564 [2:59:44<2:07:38, 73.64s/it]\u001b[A\n",
            " 82% 461/564 [3:00:50<2:02:22, 71.29s/it]\u001b[A\n",
            " 82% 462/564 [3:01:51<1:56:17, 68.41s/it]\u001b[A\n",
            " 82% 463/564 [3:02:52<1:50:55, 65.89s/it]\u001b[A\n",
            " 82% 464/564 [3:03:47<1:44:23, 62.64s/it]\u001b[A\n",
            " 82% 465/564 [3:04:42<1:39:47, 60.48s/it]\u001b[A\n",
            " 83% 466/564 [3:05:35<1:35:10, 58.27s/it]\u001b[A\n",
            " 83% 467/564 [3:06:25<1:30:17, 55.85s/it]\u001b[A\n",
            " 83% 468/564 [3:07:03<1:20:50, 50.52s/it]\u001b[A\n",
            " 83% 469/564 [3:08:25<1:34:45, 59.85s/it]\u001b[A\n",
            " 83% 470/564 [3:08:45<1:15:05, 47.93s/it]\u001b[A\n",
            " 84% 471/564 [3:10:41<1:46:04, 68.44s/it]\u001b[A\n",
            " 84% 472/564 [3:12:33<2:04:52, 81.44s/it]\u001b[A\n",
            " 84% 473/564 [3:14:23<2:16:16, 89.86s/it]\u001b[A\n",
            " 84% 474/564 [3:16:09<2:22:18, 94.88s/it]\u001b[A\n",
            " 84% 475/564 [3:17:55<2:25:42, 98.23s/it]\u001b[A\n",
            " 84% 476/564 [3:19:37<2:25:44, 99.37s/it]\u001b[A\n",
            " 85% 477/564 [3:21:17<2:24:12, 99.45s/it]\u001b[A\n",
            " 85% 478/564 [3:22:51<2:19:59, 97.67s/it]\u001b[A\n",
            " 85% 479/564 [3:24:23<2:16:06, 96.07s/it]\u001b[A\n",
            " 85% 480/564 [3:25:56<2:13:04, 95.05s/it]\u001b[A"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the model\n",
        "Run first the --help command to learn which arguments you should provide"
      ],
      "metadata": {
        "id": "OSbw6VIzaZJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python $EVALUATION_PATH --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qy3hk7wuviTP",
        "outputId": "885649e7-5591-4756-c284-24886ba0d0b7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: evaluate_model.py [-h] [--transcribe_sents TRANSCRIBE]\n",
            "                         [--evaluate_sents EVALUATE] [--data_path DATA_PATH]\n",
            "                         [--examples_basename BASENAME]\n",
            "                         [--model_folder MODEL_FOLDER]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --transcribe_sents TRANSCRIBE\n",
            "                        How many sentences to transcribe\n",
            "  --evaluate_sents EVALUATE\n",
            "                        On how many sentences to run the evaluation\n",
            "  --data_path DATA_PATH\n",
            "                        Path to the folder containing the csv file and the\n",
            "                        model folder. Defaults to empty string '' which might\n",
            "                        work if data sits in the current working directory. If\n",
            "                        it doesn't work, then you need to specify the path.\n",
            "  --examples_basename BASENAME\n",
            "                        Name of the csv file, needs to be located in the data\n",
            "                        path.\n",
            "  --model_folder MODEL_FOLDER\n",
            "                        Name of the folder containing the model, needs to be\n",
            "                        located inside the data path folder.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python $EVALUATION_PATH --transcribe_sents 5 --data_path $DATA_PATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMwDGSdfaYWT",
        "outputId": "31658191-1ae5-44ec-8b35-73a154da1276"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "06/21/2022 14:43:50 - INFO - huggingsound.speech_recognition.model - Loading model...\n",
            "06/21/2022 14:43:57 - INFO - rnc - Requested to 'https://processing.ruscorpora.ru/search.xml' [0;1) with params {'env': 'alpha', 'api': '1.0', 'lang': 'ru', 'dpp': 5, 'spd': 10, 'text': 'lexgramm', 'out': 'normal', 'sort': 'i_grtagging', 'nodia': 0, 'lex1': 'и', 'mode': 'murco'}\n",
            "06/21/2022 14:43:57 - DEBUG - rnc - Worker-1: Requested to 'https://processing.ruscorpora.ru/search.xml' with '{'env': 'alpha', 'api': '1.0', 'lang': 'ru', 'dpp': 5, 'spd': 10, 'text': 'lexgramm', 'out': 'normal', 'sort': 'i_grtagging', 'nodia': 0, 'lex1': 'и', 'mode': 'murco', 'p': 0}'\n",
            "06/21/2022 14:43:57 - DEBUG - rnc - Worker-1: Received from 'https://processing.ruscorpora.ru/search.xml' with '{'env': 'alpha', 'api': '1.0', 'lang': 'ru', 'dpp': 5, 'spd': 10, 'text': 'lexgramm', 'out': 'normal', 'sort': 'i_grtagging', 'nodia': 0, 'lex1': 'и', 'mode': 'murco', 'p': 0}'\n",
            "06/21/2022 14:43:57 - INFO - rnc - Request was successfully completed\n",
            "06/21/2022 14:43:57 - INFO - rnc - Coro executing time: 0.67\n",
            "100% 5/5 [02:06<00:00, 25.39s/it]\n",
            " Ну э́то же/ та́к же нельзя́! Что́ во́т скока достиже́ний// а скока кро́ви/ а скока же́ртв/ а скока тру́пов? Э́того не́ту/ вот е́сть вели́кие достиже́ния/ а шо за э́тими достиже́ниями го́ры тру́пов.\n",
            "Vowels only: #э##а##яоо####е####о####е###у#э##е##е#и####е#####э####е###о#у#\n",
            "Transcribed: аааааааа\n",
            " Но ты́ не заме́тишь ― слипа́ются ве́ки/ права́ свои́ бе́режно тре́бует со́н. Тебе́ бу́дут сни́ться го́ры и ре́ки/ оби́женно звя́кнет упа́вший смартфо́н.\n",
            "Vowels only: #ы##е##а##е##а#ие##е##о#еу#и#о##е##и##я##а##о\n",
            "Transcribed: аааа\n",
            " Я́ счита́ю/ что/ коне́чно/ прекра́сное ме́сто ― э́то Воробьёвы го́ры и на́бережная/ и вот всё там от па́рка Музео́н туда́ да́льше до́/ не зна́ю/ там моста́ Бережко́вского.\n",
            "Vowels only: я#а###е##а##е#э###ё#о##а######ё##а###о#аа#о#а###а##о##\n",
            "Transcribed: аааааааа\n",
            " И/ коне́чно/ смо́трят туда́/ на Росси́ю/ где́ го́ры/ сте́пи/ ле́са/ тайга́/ огро́мные города́/ огро́мные простра́нства и/ са́мое гла́вное/ тала́нтливейший наро́д.\n",
            "Vowels only: ##е#о##а##и#ео#е#е##а#о####а#о###а##а##а###а####о\n",
            "Transcribed: аааааааа\n",
            " Э́то таи́нственная/ ска́зочная страна́/ и лю́ди/ когда́ попада́ют сюда́/ к на́м/ в таку́ю сне́жно-ледо́вую ска́зку/ они́ начина́ют обраща́ть внима́ние/ что и го́ры-то зде́сь е́сть/ и они́ вокру́г на́с/ что э́то действи́тельно насто́лько краси́во.\n",
            "Vowels only: э##и###а####а#ю##а##а##аа#у#е##о##а##и##а###а#а####о##ее##и#уа#э##и###о##и#\n",
            "Transcribed: ааааааааааааа\n",
            "100% 10/10 [04:32<00:00, 27.21s/it]\n",
            "{'cer': 0.89171974522293, 'wer': 1.0}\n"
          ]
        }
      ]
    }
  ]
}