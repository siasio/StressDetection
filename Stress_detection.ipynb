{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stress-detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Library for sound processing with the Wav2Vec2 models: https://github.com/jonatasgrosman/huggingsound <br>\n",
        "API of the Russian National Corpus: https://github.com/kunansy/RNC <br>\n",
        "Not fine-tuned large Wav2Vec2 model pretrained on common_voice dataset for 53 languages: \"facebook/wav2vec2-large-xlsr-53\"<br>\n",
        "Fine-tuned Wav2Vec2 model which is most probably useless for us because it uses a token set containing characters of the cyrillic alphabet and we want to also use tokens which mark the lexical stress: \"jonatasgrosman/wav2vec2-large-xlsr-53-russian\"<br>\n",
        "Training arguments (might be useful for performing ablation studies): https://huggingface.co/transformers/v4.4.2/_modules/transformers/training_args.html"
      ],
      "metadata": {
        "id": "-0upsp9judfq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Google drive\n",
        "Mounting google drive is necessary for working with files saved there. I shared the folder with RNC data with you. To work with the data folder, go to the \"Shared with me\" folder on your google drive, right-click on the RussianNationalCorpus and press \"Add shortcut to Drive\"."
      ],
      "metadata": {
        "id": "NZkKRaECVwVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "import os\n",
        "DATA_PATH = \"/content/drive/MyDrive/RussianNationalCorpus\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jY0oHFoll87",
        "outputId": "1a0499cd-e141-4232-d4d5-8e9f8be41fa1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/vitreusx/stress.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxaOaK9BJxlp",
        "outputId": "586ec15b-e651-460e-b5ee-3a10f79db4a5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'stress'...\n",
            "remote: Enumerating objects: 17, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 17 (delta 5), reused 3 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (17/17), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DOWNLOADING_PATH = os.path.join('/content/stress/download_examples.py')\n",
        "CLEANING_PATH = os.path.join('/content/stress/clean_csv.py')\n",
        "TRAINING_PATH = os.path.join('/content/stress/run_training.py')\n",
        "EVALUATION_PATH = os.path.join('/content/stress/evaluate_model.py')"
      ],
      "metadata": {
        "id": "YtQH-2A8Jvw2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pip install libraries rnc and hugging sound\n",
        "Attention: after installing the hugginsound library, you will most probably need to change a library file called trainer.py. In colab you will find it located in usr/local/lib/python3.7/dist-packages/hugginsound/trainer.py (to find the usr folder, open the file browser in the left panel and press two dots above the sample_data folder to reach the root directory). The change you need to make, is replacing self.use_amp with self.use_cuda_amp in lines 434 and 451.\n",
        "\n",
        "After making the changes, you need to reinstall the library. In colab, you need to restart runtime first and then run the pip install command again.\n",
        "\n",
        "Making the changes is not required if you only want to run the evaluation."
      ],
      "metadata": {
        "id": "LVPjsx8hV64a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2sSIJ-OayjEZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afc52d22-4832-4e8b-b1cf-a858972f2fa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rnc\n",
            "  Downloading rnc-0.9.0-py3-none-any.whl (28 kB)\n",
            "Collecting types-ujson<4.3.0,>=4.2.1\n",
            "  Downloading types_ujson-4.2.1-py3-none-any.whl (2.0 kB)\n",
            "Collecting types-aiofiles<0.9.0,>=0.8.4\n",
            "  Downloading types_aiofiles-0.8.8-py3-none-any.whl (5.8 kB)\n",
            "Collecting aiofiles<0.9.0,>=0.8.0\n",
            "  Downloading aiofiles-0.8.0-py3-none-any.whl (13 kB)\n",
            "Collecting ujson<5.2.0,>=5.1.0\n",
            "  Downloading ujson-5.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: bs4<0.1.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from rnc) (0.0.1)\n",
            "Collecting aiohttp<3.9.0,>=3.8.1\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 11.4 MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4<4.11.0,>=4.10.0\n",
            "  Downloading beautifulsoup4-4.10.0-py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 6.0 MB/s \n",
            "\u001b[?25hCollecting lxml<4.9.0,>=4.8.0\n",
            "  Downloading lxml-4.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 37.1 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 46.0 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<3.9.0,>=3.8.1->rnc) (2.0.12)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp<3.9.0,>=3.8.1->rnc) (4.1.1)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.4 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 56.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<3.9.0,>=3.8.1->rnc) (21.4.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.7/dist-packages (from beautifulsoup4<4.11.0,>=4.10.0->rnc) (2.3.2.post1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp<3.9.0,>=3.8.1->rnc) (2.10)\n",
            "Installing collected packages: multidict, frozenlist, yarl, beautifulsoup4, asynctest, async-timeout, aiosignal, ujson, types-ujson, types-aiofiles, lxml, aiohttp, aiofiles, rnc\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "Successfully installed aiofiles-0.8.0 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 beautifulsoup4-4.10.0 frozenlist-1.3.0 lxml-4.8.0 multidict-6.0.2 rnc-0.9.0 types-aiofiles-0.8.8 types-ujson-4.2.1 ujson-5.1.0 yarl-1.7.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting huggingsound\n",
            "  Downloading HuggingSound-0.1.4-py3-none-any.whl (28 kB)\n",
            "Collecting jiwer<3.0.0,>=2.3.0\n",
            "  Downloading jiwer-2.3.0-py3-none-any.whl (15 kB)\n",
            "Collecting transformers<5.0.0,>=4.16.2\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 8.7 MB/s \n",
            "\u001b[?25hCollecting numba<0.54.0,>=0.53.1\n",
            "  Downloading numba-0.53.1-cp37-cp37m-manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 41.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2.0.0,>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from huggingsound) (1.11.0+cu113)\n",
            "Collecting datasets<2.0.0,>=1.18.3\n",
            "  Downloading datasets-1.18.4-py3-none-any.whl (312 kB)\n",
            "\u001b[K     |████████████████████████████████| 312 kB 20.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: librosa<0.9.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from huggingsound) (0.8.1)\n",
            "Collecting llvmlite<0.37.0,>=0.36.0\n",
            "  Downloading llvmlite-0.36.0-cp37-cp37m-manylinux2010_x86_64.whl (25.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.3 MB 62.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.18.3->huggingsound) (4.11.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.18.3->huggingsound) (21.3)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 14.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.18.3->huggingsound) (3.8.1)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 55.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.18.3->huggingsound) (1.3.5)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.18.3->huggingsound) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.18.3->huggingsound) (4.64.0)\n",
            "Collecting huggingface-hub<1.0.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 11.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.18.3->huggingsound) (1.21.6)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.18.3->huggingsound) (0.70.13)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.18.3->huggingsound) (6.0.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets<2.0.0,>=1.18.3->huggingsound) (0.3.5.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets<2.0.0,>=1.18.3->huggingsound) (4.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets<2.0.0,>=1.18.3->huggingsound) (3.7.1)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 49.1 MB/s \n",
            "\u001b[?25hCollecting python-Levenshtein==0.12.2\n",
            "  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein==0.12.2->jiwer<3.0.0,>=2.3.0->huggingsound) (57.4.0)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa<0.9.0,>=0.8.1->huggingsound) (1.1.0)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.9.0,>=0.8.1->huggingsound) (2.1.9)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.9.0,>=0.8.1->huggingsound) (1.6.0)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.9.0,>=0.8.1->huggingsound) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.9.0,>=0.8.1->huggingsound) (1.0.2)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa<0.9.0,>=0.8.1->huggingsound) (0.10.3.post1)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.9.0,>=0.8.1->huggingsound) (4.4.2)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa<0.9.0,>=0.8.1->huggingsound) (0.2.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets<2.0.0,>=1.18.3->huggingsound) (3.0.9)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa<0.9.0,>=0.8.1->huggingsound) (1.4.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets<2.0.0,>=1.18.3->huggingsound) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets<2.0.0,>=1.18.3->huggingsound) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets<2.0.0,>=1.18.3->huggingsound) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets<2.0.0,>=1.18.3->huggingsound) (3.0.4)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->librosa<0.9.0,>=0.8.1->huggingsound) (1.15.0)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 45.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa<0.9.0,>=0.8.1->huggingsound) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa<0.9.0,>=0.8.1->huggingsound) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa<0.9.0,>=0.8.1->huggingsound) (2.21)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.16.2->huggingsound) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 22.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.18.3->huggingsound) (6.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.18.3->huggingsound) (0.13.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.18.3->huggingsound) (21.4.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.18.3->huggingsound) (1.7.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.18.3->huggingsound) (2.0.12)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.18.3->huggingsound) (1.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.18.3->huggingsound) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0.0,>=1.18.3->huggingsound) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets<2.0.0,>=1.18.3->huggingsound) (3.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets<2.0.0,>=1.18.3->huggingsound) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets<2.0.0,>=1.18.3->huggingsound) (2022.1)\n",
            "Building wheels for collected packages: python-Levenshtein\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp37-cp37m-linux_x86_64.whl size=149873 sha256=23d0bfe1307d8bdf9ca720256adeeb615f034f4dbfb44b24ef721185256a336c\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/5f/ca/7c4367734892581bb5ff896f15027a932c551080b2abd3e00d\n",
            "Successfully built python-Levenshtein\n",
            "Installing collected packages: urllib3, llvmlite, pyyaml, numba, fsspec, xxhash, tokenizers, responses, python-Levenshtein, huggingface-hub, transformers, jiwer, datasets, huggingsound\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.34.0\n",
            "    Uninstalling llvmlite-0.34.0:\n",
            "      Successfully uninstalled llvmlite-0.34.0\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.51.2\n",
            "    Uninstalling numba-0.51.2:\n",
            "      Successfully uninstalled numba-0.51.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed datasets-1.18.4 fsspec-2022.5.0 huggingface-hub-0.8.1 huggingsound-0.1.4 jiwer-2.3.0 llvmlite-0.36.0 numba-0.53.1 python-Levenshtein-0.12.2 pyyaml-6.0 responses-0.18.0 tokenizers-0.12.1 transformers-4.20.1 urllib3-1.25.11 xxhash-3.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install rnc\n",
        "!pip install huggingsound"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download samples if they are not downloaded yet\n",
        "Perhaps, we need to import the nest_asyncio library. Without it, colab has problems with managing nested asynchronous processes."
      ],
      "metadata": {
        "id": "MtBXiOEWXeHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "words_list = ['волк', 'вертолёт']\n",
        "WORDS = ' '.join(words_list)\n",
        "!python $DOWNLOADING_PATH --help"
      ],
      "metadata": {
        "id": "rFbYH9vIynJt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24385f17-96b2-4b43-a648-b1db564284fe"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: download_examples.py [-h] [--data_path DATA_PATH]\n",
            "                            [--examples_basename BASENAME]\n",
            "                            [--helper_basename HELPER_NAME]\n",
            "                            [--media_folder MEDIA_FOLDER]\n",
            "                            [--pages_per_word PAGES_PER_WORD]\n",
            "                            words [words ...]\n",
            "\n",
            "positional arguments:\n",
            "  words                 List of the words to query the Russian National Corpus\n",
            "                        for.\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --data_path DATA_PATH\n",
            "                        Path to the directory in which the csv file and the\n",
            "                        media folder should be created. Defaults to empty\n",
            "                        string '' which might work if data sits in the current\n",
            "                        working directory. If it doesn't work, then you need\n",
            "                        to specify the path.\n",
            "  --examples_basename BASENAME\n",
            "                        Name of the csv file to write data to.\n",
            "  --helper_basename HELPER_NAME\n",
            "                        Helper csv file (will be deleted when the program\n",
            "                        finishes running).\n",
            "  --media_folder MEDIA_FOLDER\n",
            "                        Name of the folder to save the video files in.\n",
            "  --pages_per_word PAGES_PER_WORD\n",
            "                        Number of pages per word (each page contains 5\n",
            "                        examples).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python $CLEANING_PATH --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwQzrkX6LGjQ",
        "outputId": "caecfda6-11f7-426a-c6bd-0767161d6b82"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  File \"/content/stress/clean_csv.py\", line 32\n",
            "    def drop_duplicates_and_overwrite(dataframe)\n",
            "                                               ^\n",
            "SyntaxError: invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run training\n",
        "Run first the --help command to learn which arguments you should provide"
      ],
      "metadata": {
        "id": "AL-YGEGCXxyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python $TRAINING_PATH --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wp_h5aaquYDQ",
        "outputId": "c0ea00ae-621a-49ff-b45d-9106f3cf55f8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: run_training.py [-h] [--data_path DATA_PATH] [--train_data TRAIN_DATA]\n",
            "                       [--eval_data EVAL_DATA] [--use_model_from_the_web]\n",
            "                       [--model_folder MODEL_FOLDER]\n",
            "                       [--model_web_location MODEL_WEB_LOCATION]\n",
            "                       [--output_dir OUTPUT_DIR] [--not_overwrite_output_dir]\n",
            "                       [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --data_path DATA_PATH\n",
            "                        Path to the folder containing the csv file and the\n",
            "                        model folder. Defaults to empty string '' which might\n",
            "                        work if data sits in the current working directory. If\n",
            "                        it doesn't work, then you need to specify the path.\n",
            "  --train_data TRAIN_DATA\n",
            "                        Name of the csv file with the training data, needs to\n",
            "                        be located in the data path. Default: \"all_data20\" -\n",
            "                        dataset with videos no longer than 20 seconds.\n",
            "  --eval_data EVAL_DATA\n",
            "                        Name of the csv file with the evaluation data, needs\n",
            "                        to be located in the data path. Defaults: None (not\n",
            "                        using evaluation during training).\n",
            "  --use_model_from_the_web\n",
            "                        If this flag is not used, use --model_folder argument.\n",
            "                        If the flag is present, use --model_web_location\n",
            "                        argument.\n",
            "  --model_folder MODEL_FOLDER\n",
            "                        Name of the folder containing the model, needs to be\n",
            "                        located inside the data path folder. Default:\n",
            "                        \"best_model\".\n",
            "  --model_web_location MODEL_WEB_LOCATION\n",
            "                        Name of the web location containing the model, either\n",
            "                        fine-tuned or only pretrained. Note that for a fine-\n",
            "                        tuned model a token set is already defined. Default:\n",
            "                        \"facebook/wav2vec2-large-xlsr-53\" - a large model\n",
            "                        trained on 53 languages from the common_voice dataset.\n",
            "  --output_dir OUTPUT_DIR\n",
            "                        The output directory for the model. Could be the same\n",
            "                        as the --model_folder. Default: \"best_model\".\n",
            "  --not_overwrite_output_dir\n",
            "                        If this flag is provided, the --output_dir must point\n",
            "                        to an empty or non-existing directory (note that by\n",
            "                        default it points to the model folder).\n",
            "  --num_train_epochs NUM_TRAIN_EPOCHS\n",
            "                        Training will start from the next epoch compared to\n",
            "                        the one on which it finished previously. Therefore, we\n",
            "                        need to increase this number each time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the model\n",
        "Run first the --help command to learn which arguments you should provide"
      ],
      "metadata": {
        "id": "OSbw6VIzaZJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python $EVALUATION_PATH --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qy3hk7wuviTP",
        "outputId": "fa8320b2-3069-4145-8600-4dfdded497b3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: evaluate_model.py [-h] [--transcribe_sents TRANSCRIBE]\n",
            "                         [--evaluate_sents EVALUATE] [--data_path DATA_PATH]\n",
            "                         [--no_csv] [--examples_basename EXAMPLES_BASENAME]\n",
            "                         [--examples_folder EXAMPLES_FOLDER]\n",
            "                         [--model_folder MODEL_FOLDER]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --transcribe_sents TRANSCRIBE\n",
            "                        How many sentences to transcribe. 0 means: don't\n",
            "                        produce transcrtiptions. -1 means: run on all examples\n",
            "                        from the csv file.\n",
            "  --evaluate_sents EVALUATE\n",
            "                        On how many sentences to run the evaluation. 0 means:\n",
            "                        don't run the evaluation. -1 means: run on all\n",
            "                        examples from the csv file.\n",
            "  --data_path DATA_PATH\n",
            "                        Path to the folder containing the csv file and the\n",
            "                        model folder. Defaults to empty string '' which might\n",
            "                        work if data sits in the current working directory. If\n",
            "                        it doesn't work, then you need to specify the path.\n",
            "  --no_csv              With this flag, --examples_folder should be used\n",
            "                        instead of --examples_basename\n",
            "  --examples_basename EXAMPLES_BASENAME\n",
            "                        Name of the csv file, needs to be located in the data\n",
            "                        path.\n",
            "  --examples_folder EXAMPLES_FOLDER\n",
            "                        Name of the csv file, needs to be located in the data\n",
            "                        path.\n",
            "  --model_folder MODEL_FOLDER\n",
            "                        Name of the folder containing the model, needs to be\n",
            "                        located inside the data path folder.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python $EVALUATION_PATH --transcribe_sents 5 --evaluate_sents 0 --data_path $DATA_PATH --examples_basename all_data20.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMwDGSdfaYWT",
        "outputId": "825ecb5d-7b9e-478c-af1c-9685a08b861c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "06/28/2022 16:40:30 - INFO - huggingsound.speech_recognition.model - Loading model...\n",
            "06/28/2022 16:40:51 - INFO - rnc - Requested to 'https://processing.ruscorpora.ru/search.xml' [0;1) with params {'env': 'alpha', 'api': '1.0', 'lang': 'ru', 'dpp': 5, 'spd': 10, 'text': 'lexgramm', 'out': 'normal', 'sort': 'i_grtagging', 'nodia': 0, 'lex1': 'и', 'mode': 'murco'}\n",
            "06/28/2022 16:40:51 - DEBUG - rnc - Worker-1: Requested to 'https://processing.ruscorpora.ru/search.xml' with '{'env': 'alpha', 'api': '1.0', 'lang': 'ru', 'dpp': 5, 'spd': 10, 'text': 'lexgramm', 'out': 'normal', 'sort': 'i_grtagging', 'nodia': 0, 'lex1': 'и', 'mode': 'murco', 'p': 0}'\n",
            "06/28/2022 16:40:52 - DEBUG - rnc - Worker-1: Received from 'https://processing.ruscorpora.ru/search.xml' with '{'env': 'alpha', 'api': '1.0', 'lang': 'ru', 'dpp': 5, 'spd': 10, 'text': 'lexgramm', 'out': 'normal', 'sort': 'i_grtagging', 'nodia': 0, 'lex1': 'и', 'mode': 'murco', 'p': 0}'\n",
            "06/28/2022 16:40:52 - INFO - rnc - Request was successfully completed\n",
            "06/28/2022 16:40:52 - INFO - rnc - Coro executing time: 1.16\n",
            "100% 5/5 [01:09<00:00, 13.91s/it]\n",
            " Ну э́то же/ та́к же нельзя́! Что́ во́т скока достиже́ний// а скока кро́ви/ а скока же́ртв/ а скока тру́пов? Э́того не́ту/ вот е́сть вели́кие достиже́ния/ а шо за э́тими достиже́ниями го́ры тру́пов.\n",
            "Vowels only: #э##а##аоо####э####о####э###у#э##э##э#и####э#####э####э###о#у#\n",
            "Transcribed: #э##а##аоо####э####о####э###у#э##э##э#и####э#####э####э###о#у#\n",
            " Но ты́ не заме́тишь ― слипа́ются ве́ки/ права́ свои́ бе́режно тре́бует со́н. Тебе́ бу́дут сни́ться го́ры и ре́ки/ оби́женно звя́кнет упа́вший смартфо́н.\n",
            "Vowels only: #ы##э##а##э##а#иэ##э##о#эу#и#о##э##и##а##а##о\n",
            "Transcribed: #ы##э##а##э##а#иэ###э##о#эу#и#о##э##и##а##а##о\n",
            " Я́ счита́ю/ что/ коне́чно/ прекра́сное ме́сто ― э́то Воробьёвы го́ры и на́бережная/ и вот всё там от па́рка Музео́н туда́ да́льше до́/ не зна́ю/ там моста́ Бережко́вского.\n",
            "Vowels only: а#а###э##а##э#э###о#о##а######о##а###о#аа#о#а###а##о##\n",
            "Transcribed: а#а###э##а##э#э###о#о##а######о##а###о#аа#о#а###а##о##\n",
            " И/ коне́чно/ смо́трят туда́/ на Росси́ю/ где́ го́ры/ сте́пи/ ле́са/ тайга́/ огро́мные города́/ огро́мные простра́нства и/ са́мое гла́вное/ тала́нтливейший наро́д.\n",
            "Vowels only: ##э#о##а##и#эо#э#э##а#о####а#о###а##а##а###а####о\n",
            "Transcribed: ##э#о##а##и#эо#э#э##а#о####а#о###а##а##а###а####о\n",
            " Э́то таи́нственная/ ска́зочная страна́/ и лю́ди/ когда́ попада́ют сюда́/ к на́м/ в таку́ю сне́жно-ледо́вую ска́зку/ они́ начина́ют обраща́ть внима́ние/ что и го́ры-то зде́сь е́сть/ и они́ вокру́г на́с/ что э́то действи́тельно насто́лько краси́во.\n",
            "Vowels only: э##и###а####а#у##а##а##аа#у#э##о##а##и##а###а#а####о##ээ##и#уа#э##и###о##и#\n",
            "Transcribed: э##и###а####а#у##а##а##аа#у#э##о##а##и##а###а#а####о##ээ##и#уа#э##и####о##и#\n"
          ]
        }
      ]
    }
  ]
}